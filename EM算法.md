---
title: EM算法
date: 2020-04-10 10:09:20
categories:
	- 机器学习
tags:
	- EM算法
	- 统计信号处理

mathjax: true
---

## 1. 最大似然估计MLE

最大似然估计的思想是，知道模型的具体形式，现在有了观测数据，我们希望求模型的参数，使得在这组参数下，观测数据出现的概率是最大的。用数学公式表达就是

$$\hat{\theta} = argmax_{\theta} P(x;\theta)$$

当观测样本之间是相互独立的时候，一般可以写作如下的形式：

$$P(x;\theta) = \Pi_{i=0}^{N-1} p(x[i]; \theta)$$

$$\ln P(x;\theta) = \sum_{i=0}^{N-1} \ln p(x[i];\theta)$$

**为什么求对数：**

- 将连乘变为累加，求导更方便
- 概率累积会产生非常小的值，有可能超过计算机的精度。取对数之后就可以避免这个问题，如1e-50，取对数变为了-50.

所以目标就是求参数$\theta$使得上面的对数似然函数取得最大。

最大似然估计得到的参数可能是**有偏估计**。

###  最大似然估计方法的一般步骤

- 写出似然函数，需要知道模型的形式，但是参数未知
- 求其对数似然函数
- 求导令其为零，得到方程组
- 解方程组，得到最优的参数

但是有些情况下，求导的方法是行不通的。具体情况可详细见MLE的部分。

### 例子：高斯白噪声中的电平估计问题

N个相互独立的观测数据，$x[n] = A + w[n]$,$w[n]$服从$N(0, \sigma^2)$的高斯分布。求对$A$的估计。

这个问题是一个非常经典的问题，各种估计方法最终都会得到相同的解。下面使用MLE的方法进行求解。

1. 写出似然函数。要估计的参数是$A$， 所有有$p(x[n]; A) \sim N(A, \sigma^2)$

2. 求对数似然函数

   由于各个观测量之间是相互独立的，所以有$P(x;A) = \Pi_{n=0}^{N-1} p(x[n];A) = (\frac{1}{\sqrt{2\pi} \sigma})^N e^{-\frac{\sum_{n=0}^{N-1} (x[n] - A)^2}{2\sigma^2}}$

   取对数有$\ln P(x; A) = -\frac{\sum_{n=0}^{N-1} (x[n] - A) ^ 2}{2\sigma^2} + C$

3. 求导，得$\frac{\partial \ln P(x;A)}{\partial A}= \frac{\sum_{n=0}^{N-1} x[n] - NA}{\sigma^2} = 0$
4. 求解得到$\hat{A} = \frac{1}{N} \sum_{n=0}^{N-1} x[n]$, 为观测数据的均值。

## EM算法

### 高斯混合模型

对于求解最大似然估计，直接对待优化参数求导，令其为零，有时候并不奏效。有些模型要想得到解析解是比较困难的。比如高斯混合模型

$$P(x) = \sum_{i=1}^{k} \pi_{i} N(\mu_i, \Sigma_{i})$$

对于高斯混合模型，我们可以看做是具有不可观测的隐变量的模型，真实的采样情况是，我们有$k$个高斯分布，每个高斯分布具有的自己的参数,模型全部的参数为$\pi, \mu, \Sigma$。同时每个高斯分布被采样到的概率服从$\pi$。首先对$\pi$进行采样，决定选取哪一个高斯分布，然后再从得到的高斯分布中采样。最终得到样本点。$X$

真实的数据应该是$(x, y)$,其中$y$，表示这个样本点是从哪个高斯分布中采样得到的。如果能够拿到每个样本点所属的高斯成分类别，那么对参数的估计就是比较简单的。

1. 写出似然函数$P(x, y) = \pi_yN(\mu_y, \Sigma_y)$,各个样本之间相互独立，所有就有$P(X,Y) = \Pi_{i=1}^{N} \pi_{y_i} e^{-\frac{(x_i - \mu_{y_i})^2}{2\sigma_i^2}}$

2. 求解对数似然函数

   $$\ln P(X, Y) = \sum_{i=1}^{N} \ln \pi_{y_i} + \sum_{i=1}^{N} -\frac{(x_i - \mu_i)^2}{2\sigma_i^2}$$

3. 分别对模型的参数求导。由于模型还有一个约束是$\sum_{i=1}^{k} \pi_i = 1$。所以引入拉格朗日乘子，优化函数为$\\sum_{i=1}^{N} \ln \pi_{y_i} + \sum_{i=1}^{N} -\frac{(x_i - \mu_i)^2}{2\sigma_i^2} + \lambda (\sum_{i=1}^{k} \pi_i - 1)$

4. 求解。由于$\mu_i, \sigma_i$直接没有关系，所以优化后面的部分，就可以利用前面的结论，可以得到的结论是，每一类高斯分布的均值就是属于这一类样本的均值，方差的估计尽量就是属于这一类样本的数据的方差。所以只需要对$\pi_i$进行估计就可以。直观上来说，$\pi_i$的估计量应该是所有样本中属于第$i$类的样本个数占所有样本的比例。下面可以进行理论的推导。假设样本中$y = i$的个数为$n_i$，

   $$\frac{\partial}{\partial \pi_i} = \frac{n_i}{\pi_i} + \lambda = 0$$

   $$\lambda \pi_i = - n_i$$

   $$\lambda = - \sum_{i=1}^{k} n_i = - N$$

   $$\pi_i = - \frac{n_i}{\lambda} = -\frac{n_i}{N}$$

   得到的结果和直觉是相符合的。

**总结**：

对于高斯混合模型，如果我们拿到了每个样本其属于的高斯分量的标签，那么利用MLE的方法是可以直接得到解析解的。

- 每个分量出现的概率的估计就是样本中属于这一个分量的个数除以总样本数
- 每个分量的均值就是属于这个分量的样本的均值
- 每个分量的协方差均值就是属于这个分量的样本的协方差矩阵



但是在真实的场景中，我们可能只是假设其是若干个高斯混合得到的，我们并不能拿到每个样本所属的分量的标签。也就是上面的数据中的$y$是缺失了的。或者说具有不可观测的隐变量。对于这样的问题，其似然函数就是如下的形式:

$$P(x) = \sum_{y} P(x, y) = \sum_{i=1}^{k} \pi_i N(\mu_i, \sigma_i^2)$$

对数似然函数不难可以得到如下的形式:

$$\ln P(X) = \sum_{n=1}^N \ln \sum_{i=1}^{k} \pi_i N(\mu_i, \sigma_i^2)$$

对数似然函数中存在着$\ln$函数中求和的形式。这样的函数没有办法通过求导一步得到解析解。那么如何求解这样的问题呢？

### 问题的形式

有这样一类问题，问题中具有一些不可得到的隐变量或者数据中存在着缺失。我们希望对利用观测到的数据对模型的参数进行估计。**EM算法的流程如下**：

- 随机初始化模型的参数$\theta_0$。
- E步，利用现有的参数和观测数据，求取对于每一个数据，其未观测到的或者缺失的数据的条件分布
- 然后利用得到的条件分布，对于每个样本，求其在隐变量或者缺失数据的条件分布下，对数似然函数的期望$E_{z \sim p(z; x_i, \theta_t)}(\ln P(x_i, z; \theta))$。可以看到这一步将$\log$函数和求和交换了次序。同时由于条件分布是已知的，也就是对于每一个样本，都有一个其未观测到的隐变量的分布。然后这一步积分就相当于把隐变量给积掉了。所以这一步就是E步，求期望
- M步，对上面每个样本的结果求和，然后求导，零其最大，就可以得到$\theta_{t+1}$的值。
- 然后回到E步，直到待估计参数收敛或者达到最大的迭代步数，停止算法。

证明算法正确性之前，可以先通过几个例子来直观感受一下EM算法到底在干啥。

#### k-means

在进行`k-means`聚类的时候，我们的操作是。

- 指定聚类的个数，初始化聚类中心
- 计算每个样本到每个聚类中心的距离，然后将其判定为距离最近的那个中心的类别
- 利用上一步可以对每个样本得到一个其属于的类别，然后在利用这个类别，重新计算每一个类别的中心
- 回到第二步，直到收敛。

从EM算法的角度来看，在E步，`k-means`算法相当于一个硬分类。它直接就把样本判定为距离最近的类，而不是得到其属于各个类的概率分布，相当于E步。然后利用这个打上的标签，再对参数重新进行估计，相当于M步。

#### 混合高斯模型未观测到隐变量

- 随机初始化参数，包括$\pi, \mu, \Sigma$
- 通过现在的模型的参数，计算每一个样本，其隐变量的概率分布。也就是对每个样本而言，其属于各个分量的概率。可以使用贝叶斯公式来计算
- 写出每个样本在其隐变量的条件分布下的对数似然函数的均值。然后优化。具体的结果如下：
  - 假设有N个样本，第i个样本属于第k个分量的概率为$p_i^k$
  - 那么$\pi_k^t = \frac{\sum_{i=1}^Np_i^k}{N}$
  - $\mu_k^t = \frac{\sum_{i=1}^N{p_i^k x_i}}{\sum_{i=1}^{N} p_i^k}$
  - $\Sigma_k^t = \frac{ \sum_{i=1}^{N} p_i^{k} (x - \mu_k)(x - \mu_k)^T }{\sum_{i=1}^{N} p_i^k}$
- 上述公式的直观解释：
  - 判断每个分量出现的概率，那么就看每个数据属于这个分量的概率，然后求平均
  - 估计每个分量的均值，那么就对每个数据样本属于这个类的概率进行归一化，然后对所有样本平均
  - 判断每个分量的协方差，那么就对每个数据样本属于这个类的概率就行归一化，并用这个类估计的均值和这个数据样本计算协方差矩阵。然后再取平均。

#### 观测数据缺失的高斯分布参数估计

> 现在有N个观测到的数据，M个未观测到的数据， 数据之间是独立同分布的。已知数据满足均值为A, 方差为$\sigma^2$的高斯分布求
>
> 1. 若$\sigma^2$ 是已知的，利用EM算法求A的迭代估计
> 2. 若A和$\sigma^2$均未知，利用EM算法求两个参数的迭代估计

直观来看，由于各个数据之间是独立同分布，缺失数据应该是不能够提供任何信息。所以最终的估计量应该是观测到的数据的均值和方差。下面利用EM算法推导迭代估计的公式。

所有的观测数据是$X$, 其可以分为两部分，一部分是观测到的数据$X_{obs}$,另一部分是未观测到的数据$X_{miss}$，所以我们优化的目标是

$$\int p(X_{obs}, X_{miss};\theta)d X_{miss}$$

上述可以看做是具有隐变量的$X_{miss}$。然后我们直接利用EM公式的方法

1. 随机初始化参数，$A_{0}$

2. 在当前参数下，求确实数据的条件分布，为$P(X_{miss}| X_{obs}; A_t) = P(X_{miss};A_t) = N(A_t, \sigma^2)$

3. 求对数似然函数在条件分布下的期望$E = \int P(X_{miss}; A_t) \ln P(X_{miss}, X_{obs}; A) dX_{miss} = \int (\frac{1}{\sqrt{2\pi}\sigma})^M e^{-\frac{\sum_{i=1}^{M}(x_i - A_t)^2}{2\sigma^2}} \ln ((\frac{1}{\sqrt{2\pi} \sigma})^{M+N} e^{-\frac{\sum_{i=1}^{M} (x_i - A)^2 + \sum_{j=1}^{N}(y_j - A)^2}{2\sigma^2}}) dX_{miss}$

   其中用$X_i$表示所有的缺失样本，$y_j$表示所有的观测到的样本

   上面就是我们需要最大化的函数。可以先不直接积出来，直接对要优化的参数A求导，可以得到如下的公式

$$\int (\frac{1}{\sqrt{2\pi}\sigma})^M e^{-\frac{\sum_{i=1}^{M} (x_i - A_t)^2}{2\sigma^2}} [\frac{\sum_{i=1}^{M} x_i - MA}{\sigma^2} + \frac{\sum_{j=1}^{N} y_j - NA}{\sigma^2}] dX_{miss} = 0$$

$$A_{t+1} = \frac{M}{M+N} A_t + \frac{N}{M+N} \bar{y}$$

可以看到上述的结果是，每一次估计量是上一步估计量和观测到的数据的均值的加权求和。可以证明最终估计量会收敛到观测到的数据的均值$\bar{y}$上。

如果是均值和方差均未知，那么和上面类似的方法，只不过是在M步的时候需要分别对均值和方差求导。最终得到的A的估计量任然不变，方差的估计量的形式如下所示：



$$\sigma_{t+1}^{2}=\frac{M A_{t}^{2}+M \sigma_{t}^{2}-2 M A_{t} A_{t+1}+M A_{t+1}^{2}+\sum_{i=1}^{N}\left(y_{j}-\bar{y}\right)^{2}}{M+N}$$



不难证明最终$\sigma^2$ 会收敛到$\frac{1}{N}\sum_{j=1}^{N}(y_j - \bar{y})^2$, 和直接用观测到的数据做MLE的结果是相同的。

## EM算法理论推导

TODO

